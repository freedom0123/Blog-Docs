---
title: Kafka
date: 2023-02-14 23:21:43
permalink: /pages/3acb47/
categories:
  - 消息队列
tags:
  - 
author: 
  name: Hao Long
---
# 第3节 Kafka

## 一、初识 Kafka

### 1.1 基本概念

Kafka，是一个**分布式**的基于**发布订阅模式**的消息队列

- 发布和订阅的对象是主题
- 向这个主题之中，提供内容的是生产者
- 订阅主题，从主题之中获取内容的是消费者

在一个Kafka 体系结构中，包括：生产者，消费者，Broker，Zookeeper 集群组成。

- 生产者生产消息，并将消息推送到 Kafka 之中
- 消费者从 Kafka 之中获取消息进行消费
- Broker 就是一个节点，一个节点表示一个Kafka服务器，多个Broker 构成一个集群。将不同的 Broker 分散运行在不同的机器之上，如果一台机器出现了宕机，其他机器上的Broker也依旧可以对外提供服务，这就是 Kafka 实现高可用的手段之一。
- 通过 Zookeeper 来进行 Broker 的管理

![](http://www.haolong.xyz/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230318002653.jpg)

为了将消息进行分类，Kafka之中引入了主题的概念，生产者向指定主题之中添加内容，消费者从指定主题之中获取内容。一个主题，可以被多个消费者订阅，一个消费者，可以同时订阅多个主题。

主题也只是一个逻辑上的概念，他又可以细分为多个分区。每个分区是一组有序的消息日志，生产者根据分区的规则，只会将每条消息发送到一个分区，并且进入分区，会有一个唯一标识 offset，用来保证消息在分区内的顺序性。

在 Kafka 之中，为分区引入了多副本机制，通过增加副本数量可以提升容能力，也就说将相同的数据拷贝到不同的机器之上。在Kafka之中，有两种类型的副本：

- 领导者副本：与客户端进行交互
- 追随者副本：不与外界进行交互

工作机制主要是：生产者向领导者副本写消息，而消费者总是从领导者副本这里读取消息。而追随者副本，只需要做一件事情，就是向领导者副本发送请求，请求领导者把最新生产的消息发送给他，这样就能够保证与领导者的同步工作（在同一时刻，副本之间并不是完全相同的）。当 领导者宕机之后，从追随者之中选出新的领导者提供服务。

通过副本机制，在生产者方面，提供了一定程度的容灾能力。而在消费者方面，也具备一定的容灾能力，消费者通过拉模式从服务端拉取消息，并且会保存消费的具体位置，保证在宕机之后恢复上线时，根据保存的消息重新消费。这也说明了Kafka之中，消费者如果消费完消息之后，不会进行消息的删除。

### 1.2 安装

1）安装 JDK

2）安装 Zookeeper

3）Kafka 的安装与配置

```markdown
https://hub.docker.com/r/bitnami/kafka
```

4）查看服务进程是否已经启动

```markdown
jps -l
```

通过这个命令仅仅能够用来确认 Kafka 服务的进程已经能够正常的启动，但是是否能够正确的对外提供服务，还需要通过发送和消费消息来进行验证

使用脚本测试消息的订阅和接受

0）创建主题

```markdown
bin/kafka-topics.sh --zookeeper localhost:2181/kafka --create --topic haolong --replication-factor 1 --partitions 4
```

1）订阅主题

```markdown
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic haolong
```

2）向主题发送消息

```markdown
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic haolong
```

### 1.3 案例

生产者

```java
public class Producer {
    public static final String brokerList = "101.37.33.112:9092";
    public static final String topic = "test";
    public static Properties init() {
        Properties properties = new Properties();
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
        properties.put("bootstrap.servers",brokerList);
        return properties;
    }
    public static void main(String[] args) {
        Properties properties = init();
        // 配置生产者客户端参数并创建 KafkaProducer 实例
        KafkaProducer<String, String> producer = new KafkaProducer<>(properties);
        // 构建需要发送的消息
        ProducerRecord<String, String> record = new ProducerRecord<String, String>(topic,"Hello,Kafka");
        // 发送消息
        producer.send(record);
        // 关闭客户端实例
        producer.close();
    }
}

```

## 二、生产者

一个正常的生产逻辑需要具备以下几个步骤：

- 配置生产者客户端参数及创建相关的生产者案例
- 构建待发送的消息
- 发送消息
- 关闭生产者实例

### 2.1 消息对象

对于生产者中，发送的消息是`ProduceRecord`，这里面其实还有很多的参数：

```java
public class ProducerRecord<K, V> {
    private final String topic; // 主题
    private final Integer partition; // 分区号
    private final Headers headers; // 消息头部
    private final K key; // 键
    private final V value; // 值
    private final Long timestamp; // 消息的时间戳
}
```

对于这些属性，在构建的时候并不是都需要的，通过他的构造函数，能够发现，`topic`  和  `value` 是两个必须的参数（在所有的有参构造函数之中都具有）。topic 标志着这条消息要发送到那个主题里面，value 则对应着具体的消息值。

### 2.2 必要的参数配置

在创建真正的生产者实例前都需要配置相应的参数，在 Kafka 生产者之中，有 3 个参数是必填的

```java
public static final String brokerList = "101.37.33.112:9092";

public static Properties init() {
    Properties properties = new Properties();
    properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
    properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
    properties.put("bootstrap.servers",brokerList);
    return properties;
}

Properties properties = init();
// 配置生产者客户端参数并创建 KafkaProducer 实例
KafkaProducer<String, String> producer = new KafkaProducer<>(properties);
```

对于 `bootstrap.servers`，这个参数用来指定生产者客户端连接 Kafka 集群所需要的broker 清单，中间使用逗号隔开。

对于 broker 端接受的消息必须以字节数组的形式存在，所以在发送前需要将 key 和 value 进行序列化 。

在创建消息的发送者 `KafkaProducer` 对象的时候，指定的两个泛型，分别对应 消息实体中 key 和 Value 的类型

KafkaProducer 是线程安全的，可以在多个线程中共享单个 KafkaProducer 实例，所以我们创建多个 KafkaProducer 对象，池化。

### 2.3 消息的发送方式

在Kafka 之中，主要有三种消息发送模式：发后即忘、同步、异步

#### 01 发后即忘

发后即忘，只管发送，并不关心是否正确的到达，这种方式的性能是最高的，但是可靠性确实最差的。

#### 02 异步

实际上，`send()` 方法本身就是异步的

#### 02 同步

同步，就是发送消息之后，我就一直等着，知道这个消息成功到达了，我在进行后续的操作，如果说发生了异常，那么就需要捕获异常，进行相应的处理，比如：重发，记录日志等。

这种方式的实现，主要依靠于`send` 方法。

```java
public Future<RecordMetadata> send(ProducerRecord<K, V> record) {
    return this.send(record, (Callback)null);
}

public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
    ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);
    return this.doSend(interceptedRecord, callback);
}
```

这两个方法的返回值都是 `Future` 对象，这个对象就表示一个任务的生命周期，并提供了相应的方法来判断任务是否已经完成了。

![image-20230303223325974](http://www.haolong.xyz/image-20230303223325974.png)

```java
Future<RecordMetadata> future = producer.send(record);
future.get();
```

而执行的结果就是：`RecordMetadata`，在这里面包含了消息的一些信息

```java
public final class RecordMetadata {
    public static final int UNKNOWN_PARTITION = -1;
    private final long offset;
    private final long timestamp;
    private final int serializedKeySize;
    private final int serializedValueSize;
    private final TopicPartition topicPartition;
}

public final class TopicPartition implements Serializable {
    private static final long serialVersionUID = -613627415771699627L;
    private int hash = 0;
    private final int partition;
    private final String topic;
}
```

### 2.4 异常类型

KafkaProducer 中一般会发生两个类型的异常：`可重试异常` 和 `不可重试的异常`。

对于可重试异常，如果配置了 retries 参数，在这个重试次数范围之内，如果说发送成功了，就不会抛出异常

```java
properties.put(ProducerConfig.RECEIVE_BUFFER_CONFIG,6);
```

### 2.5 序列化

生产者需要用 **序列化器** 把对象转化字节数组才能通过网络发送给Kafka。而消费者需要用对应的**反序列化器**把从 Kafka 之中收到的字节数组转换成了相应的对象。

而在我们的代码示例之中，使用的是 String 类型的序列器

```java
properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
```

这个时候，我们首先来看一下这个序列化器，这里省略了对应的方法体内容

```java
public class StringSerializer implements Serializer<String> {
    
    private String encoding;
    
    public StringSerializer() {
        this.encoding = StandardCharsets.UTF_8.name();
    }
	// 用来配置当前类
    public void configure(Map<String, ?> configs, boolean isKey) {}
    
	// 用来执行序列化操作
    public byte[] serialize(String topic, String data) {}
}
```

接下来，看一下接口中提供的方法，对于这个 close 方法，一般情况之下是一个空方法，如果实现了这个方法，则必须保证此方法的幂等性，因为这个方法可能被 KafkaProducer 调用多次

```java
public interface Serializer<T> extends Closeable {
    default void configure(Map<String, ?> configs, boolean isKey) {}

    byte[] serialize(String var1, T var2);

    default byte[] serialize(String topic, Headers headers, T data) {
        return this.serialize(topic, data);
    }

    default void close() {}
}
```

### 2.6 生产者拦截器

实现 `ProducerInterceptor` 接口

### 2.7 分区器

消息经过序列化之后，首先会通过拦截器，序列化器，分区器的一系列操作之后，才会被真正的发送到 Broker 之中。如果说消息中指定了分区字段`partition`，就不需要去走分区器了，如果说没有指定，就需要依赖分区器，根据key这个字段来计算`partition`，分区器的作用就是为消息分配分区。

### 2.8 API

## 三、消费者


